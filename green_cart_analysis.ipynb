{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8faf4304",
   "metadata": {},
   "source": [
    "# ðŸ›’ Sales & Customer Behaviour Insights â€“ Green Cart Ltd.\n",
    "**Analysis using Jupyter (Python) and SQL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b108ba",
   "metadata": {},
   "source": [
    "This notebook explores sales and customer behaviour for Green Cart Ltd., a UK-based eco-friendly e-commerce company. The goal is to provide actionable insights to support marketing and operational decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8fe0f5",
   "metadata": {},
   "source": [
    "## ðŸ“¥ Task 1: Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16868a6",
   "metadata": {},
   "source": [
    "We begin by importing the necessary libraries and loading the three datasets into separate DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb4a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sales_df = pd.read_csv(\"sales_data.csv\")\n",
    "product_df = pd.read_csv(\"product_info.csv\")\n",
    "customer_df = pd.read_csv(\"customer_info.csv\")\n",
    "\n",
    "print(\"âœ… Data loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec390cf",
   "metadata": {},
   "source": [
    "## ðŸ§¹ Task 2: Clean the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749c64d6",
   "metadata": {},
   "source": [
    "Data cleaning includes standardising text format, converting date columns, handling missing values, and removing duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e64cadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise text formatting\n",
    "sales_df['delivery_status'] = sales_df['delivery_status'].str.strip().str.title()\n",
    "sales_df['payment_method'] = sales_df['payment_method'].str.strip().str.title()\n",
    "sales_df['region'] = sales_df['region'].str.strip().str.title()\n",
    "\n",
    "customer_df['gender'] = customer_df['gender'].str.strip().str.title()\n",
    "customer_df['region'] = customer_df['region'].str.strip().str.title()\n",
    "customer_df['loyalty_tier'] = customer_df['loyalty_tier'].str.strip().str.title()\n",
    "\n",
    "# Convert dates\n",
    "sales_df['order_date'] = pd.to_datetime(sales_df['order_date'], errors='coerce')\n",
    "product_df['launch_date'] = pd.to_datetime(product_df['launch_date'], errors='coerce')\n",
    "customer_df['signup_date'] = pd.to_datetime(customer_df['signup_date'], errors='coerce')\n",
    "\n",
    "# Handle missing values\n",
    "sales_df['discount_applied'] = sales_df['discount_applied'].fillna(0.0)\n",
    "sales_df = sales_df.dropna(subset=['order_id', 'customer_id', 'product_id', 'order_date'])\n",
    "\n",
    "customer_df['loyalty_tier'] = customer_df['loyalty_tier'].fillna('Unknown')\n",
    "customer_df['region'] = customer_df['region'].fillna('Unknown')\n",
    "customer_df['email'] = customer_df['email'].fillna('unknown@unknown.com')\n",
    "\n",
    "# Remove duplicates\n",
    "sales_df = sales_df.drop_duplicates(subset=['order_id'])\n",
    "product_df = product_df.drop_duplicates(subset=['product_id'])\n",
    "customer_df = customer_df.drop_duplicates(subset=['customer_id'])\n",
    "\n",
    "# Validate numeric fields\n",
    "sales_df['quantity'] = pd.to_numeric(sales_df['quantity'], errors='coerce')\n",
    "sales_df = sales_df[sales_df['quantity'] >= 0]\n",
    "sales_df = sales_df[sales_df['unit_price'] >= 0]\n",
    "sales_df = sales_df[sales_df['discount_applied'].between(0, 1)]\n",
    "\n",
    "print(\"âœ… Data cleaned and standardised\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80b2ba7",
   "metadata": {},
   "source": [
    "## ðŸ”— Task 3: Merge the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfedafe9",
   "metadata": {},
   "source": [
    "We now merge the three cleaned DataFrames to create a unified view of the sales, customer, and product data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38be06a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sales with products, then with customers\n",
    "merged_df = sales_df.merge(product_df, on='product_id', how='left')\n",
    "merged_df = merged_df.merge(customer_df, on='customer_id', how='left')\n",
    "\n",
    "# Quick check\n",
    "print(\"âœ… Merged data shape:\", merged_df.shape)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a67b6fe",
   "metadata": {},
   "source": [
    "## ðŸ§  Task 4: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2769f6d",
   "metadata": {},
   "source": [
    "We derive new features to enrich the analysis and extract business insights. These include revenue, price bands, and customer email domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa69a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features\n",
    "merged_df['revenue'] = merged_df['quantity'] * merged_df['unit_price'] * (1 - merged_df['discount_applied'])\n",
    "merged_df['order_week'] = merged_df['order_date'].dt.isocalendar().week\n",
    "merged_df['price_band'] = pd.cut(\n",
    "    merged_df['unit_price'],\n",
    "    bins=[0, 15, 30, float('inf')],\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "merged_df['days_to_order'] = (merged_df['order_date'] - merged_df['launch_date']).dt.days\n",
    "merged_df['email_domain'] = merged_df['email'].str.extract(r'@(.+)$')\n",
    "merged_df['is_late'] = merged_df['delivery_status'] == \"Delayed\"\n",
    "\n",
    "# Preview engineered dataset\n",
    "merged_df[['revenue', 'order_week', 'price_band', 'days_to_order', 'email_domain', 'is_late']].head()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
